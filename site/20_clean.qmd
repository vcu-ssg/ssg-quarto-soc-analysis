---
title: Cleaning the data
---

This script describes the data and walks through a process of data cleaning.

At completion of this script all the individual semester data files will be combined into a single dataframe for analysis, and the dataframe will be refactored for cleaner analysis.

## Data sources

The files are named appropriately and stored in the *data* folder in this repository.

```{python}
#| echo: false
import os
import sys
import json
import numpy as np
import pandas as pd
import gspread
import pathlib
import matplotlib.pyplot as plt
from IPython.display import Markdown
from tabulate import tabulate
from gspread_dataframe import set_with_dataframe, get_as_dataframe
from oauth2client.service_account import ServiceAccountCredentials

directory_path = './data'
file_list = os.listdir(directory_path)
xlsx_files = [file for file in file_list if file.endswith('.xlsx')]
xlsx_df = pd.DataFrame({'file_name':xlsx_files})

xlsx_df['term_code'] = xlsx_df['file_name'].str.split('-').str[2]

def convert_ay(term_code):
    year = term_code[2:4]
    return f'AY{int(year)-1}-{year}'

xlsx_df['academic_year'] = xlsx_df['term_code'].apply(convert_ay)

xlsx_df['period_code'] = xlsx_df['file_name'].str.split('-').str[3]
xlsx_df['period_code'] = xlsx_df['period_code'].str.split('.').str[0]

def convert_period(period_code):
    year = period_code[2:]
    period_id = period_code[:2]
    terms = {'FA': 'Fall', 'SP': 'Spring', 'SU': 'Summer'}
    period_name = terms.get(period_id, 'Unknown')
    return f'{period_name} {year}'

xlsx_df['period_name'] = xlsx_df['period_code'].apply(convert_period)

Markdown(tabulate(
  xlsx_df, 
  headers=['File name','Term Code','Acad Year','Period Code','Period Name'],
  numalign="left",stralign="left",
  showindex=False
))

```

## Data Cleaning

The following sections combine the individual sources of data and clean them up.

### Combining to single data frame

The following section combines each XLSX into a single data frame.  The contents of the files are variable in length depending on the number of sections taught
during any given semester.

The appropriate block starts one row after the row with the keyword *TERM* in the first column, denoting the header row.  The appropriate block ends with the row prior to the row with the keyword *N =*. 

We're assuming that all xlsx workbooks share the same columns and names.

```{python} 
#| echo: true
#| 
# Initialize an empty list to store trimmed DataFrames
trimmed_dfs = []

# Iterate through the list of XLSX file names
for file_name in xlsx_df['file_name']:
    # Load the XLSX file into a DataFrame
    full_path = os.path.join(directory_path, file_name)
    df = pd.read_excel(full_path)
    
    # Find the row index where "TERM" is in the first column
    term_index = df.index[df.iloc[:, 0] == "TERM"].tolist()[0]
    
    # Find the row index where "N=" is in the first column
    n_index = df.index[df.iloc[:, 0].str.startswith("N =").fillna(False)].tolist()[0]

    # Clip the desired block and add column names
    trimmed_df = df.loc[term_index + 1 : n_index - 1]
    trimmed_df.columns = df.iloc[term_index].values

    # Append the trimmed DataFrame to the list
    trimmed_dfs.append(trimmed_df)

# Combine the individual dataframes into one big one.
sections_df = pd.concat(trimmed_dfs, ignore_index=True)
```

### Merge in the term and period data from the xlsx_df dataframe

The following code merges in the term and period data with the section data.
First ensure that the key columns are strings, then merge away.

```{python}
sections_df['SECT'] = sections_df['SECT'].astype(str)
sections_df['TERM'] = sections_df['TERM'].astype(str)
xlsx_df['term_code'] = xlsx_df['term_code'].astype(str)
sections_df = pd.merge(sections_df,xlsx_df,left_on='TERM', right_on='term_code', how='left')
```

### Fix known errors

The data in Banner doesn't always reflect reality.  This step corrects
known errors in the data.

Note that the section data are stored one row per term-crn-meeting period.

```{python}
#| echo: false

# Ensure that columns TERM and CRN are strings to help with lookup.

sections_df['TERM'] = sections_df['TERM'].astype(str)
sections_df['CRN'] = sections_df['CRN'].astype(str)

sections_df["fixes"] = ""
sections_df["fix_notes"] = ""

# A helper function to correct data.

def fix( id, note, key, data ):
    for item in data.keys():
        sections_df.loc[(sections_df["TERM"]==key["TERM"])&(sections_df["CRN"]==key["CRN"]),item]=data[item]
    sections_df.loc[(sections_df["TERM"]==key["TERM"])&(sections_df["CRN"]==key["CRN"]),"fixes"] += f";FIX{id:03}"
    sections_df.loc[(sections_df["TERM"]==key["TERM"])&(sections_df["CRN"]==key["CRN"]),"fix_notes"] += ";" + note

```

```{python}
# Spring 2023: CMSC475 John Leonard taught for David Shepherd
fix(1,'Swap Leonard for Shepherd',{'TERM':'202320','CRN':'43471'},{'PRIMARY INSTRUCTOR FIRST NAME':'John','PRIMARY INSTRUCTOR LAST NAME':'Leonard'})

# Fall 2024: CMSC391 is cross listed with COAR463.
# Total enrollment 34 across two instructors (Bennett and Leonard)
# Currently Banner shows only 20 in the JL section and doesn't mention the COAR section.
fix(2,'Fix incorrect cross listing',{'TERM':'202410','CRN':'46263'},{'ACTUAL ENROLLMENT':34})

#fix({'TERM':'202320','CRN':'43471'},{'PRIMARY INSTRUCTOR FIRST NAME':'John','PRIMARY INSTRUCTOR LAST NAME':'Leonard'})


```

### Reshape the data

The current dataframe *sections_df* contains one record per term-crn-meeting period tuple.  Within each tuple there can be up to 2 instructors.  We need to
reshape the data with the instructors in a single column.

While we're here we can combine instructor first and last name, and drop verbose columns.

```{python}
cols = sections_df.columns
values_to_remove = ['PRIMARY INSTRUCTOR FIRST NAME','PRIMARY INSTRUCTOR LAST NAME','SECONDARY INSTRUCTOR FIRST NAME','SECONDARY INSTRUCTOR LAST NAME']
cols = [x for x in cols if x not in values_to_remove]

sections_df['ins1_last'] = sections_df['PRIMARY INSTRUCTOR LAST NAME']
sections_df['ins1_first'] = sections_df['PRIMARY INSTRUCTOR FIRST NAME']
sections_df['ins2_last'] = sections_df['SECONDARY INSTRUCTOR LAST NAME']
sections_df['ins2_first'] = sections_df['SECONDARY INSTRUCTOR FIRST NAME']
sections_df['instructor_1'] = sections_df['ins1_last']+','+sections_df['ins1_first']
sections_df['instructor_2'] = sections_df['ins2_last']+','+sections_df['ins2_first']
sections_df['instructor_1'].fillna('',inplace=True)
sections_df['instructor_2'].fillna('',inplace=True)

stacked_df = pd.melt(sections_df,
    id_vars=cols,
    value_vars=['instructor_1','instructor_2'],
    var_name='instructor source',
    value_name='instructor'
)
```

### Clean up rows

```{python}
# remove rows with empty instructor 1.  Keep rows with empty instructor 1.
stacked_df = stacked_df[ ~ ((stacked_df['instructor source']=='instructor_2') & (stacked_df['instructor']=='')) ]

# replace any missing instructors with note
stacked_df.loc[stacked_df["instructor"].isin(['']),"instructor"] = stacked_df[stacked_df["instructor"].isin([''])]['COURSE']+' '+stacked_df[stacked_df["instructor"].isin([''])]['TERM']+' '+stacked_df[stacked_df["instructor"].isin([''])]['CRN']

# drop rows with zero enrollments
stacked_df = stacked_df[stacked_df['ACTUAL ENROLLMENT']>0]

# Sort the data frame so it looks pretty in the output file.
stacked_df = stacked_df.sort_values(['TERM','DEPT','COURSE','SECT','instructor source','instructor'])

```

### Add additional columns

We may need the subject and the course number in the workload model analysis.

```{python}
stacked_df["course_subject"] = stacked_df["COURSE"].str[:4]
stacked_df["course_number"] = stacked_df["COURSE"].str[4:]
```

### Merge persistent instructor data

We're storing some of the results in google sheets.  Further, we're using google
sheets to store persistent data not found in the VCU course schedule report.

```{python}
# define scope
scope = ['https://spreadsheets.google.com/feeds', 'https://www.googleapis.com/auth/drive']

# create credentials object
credential_file = os.path.join(os.path.expanduser("~"), ".gsecrets", "gsheets-credentials.json")
if not os.path.isfile( credential_file ):
  print("Missing credential file:",credential_file)
  sys.exit()

# authorize the client
creds = ServiceAccountCredentials.from_json_keyfile_name(credential_file, scope)
client = gspread.authorize(creds)

spreadsheet_key = "1ZK7k8M85CXLof6FdeJYJuGFbfjsOXrCv5mc7OgUInWw"
worksheet_name = "Instructor data"
sheet = client.open_by_key(spreadsheet_key).worksheet(worksheet_name)
instructor_df = get_as_dataframe(worksheet=sheet, evaluate_formulas=True )

stacked_df = pd.merge( stacked_df,instructor_df,left_on='instructor',right_on='instructor',how='left')

worksheet_name = "Instructor notes"
sheet = client.open_by_key(spreadsheet_key).worksheet(worksheet_name)
instructor_notes_df = get_as_dataframe(worksheet=sheet, evaluate_formulas=True )

stacked_df = pd.merge( stacked_df,instructor_notes_df,left_on='instructor',right_on='instructor',how='left')

```

### Merge persistent course data

```{python}
# spreadsheet_key = "1ZK7k8M85CXLof6FdeJYJuGFbfjsOXrCv5mc7OgUInWw" from above!
worksheet_name = "Course notes"
sheet = client.open_by_key(spreadsheet_key).worksheet(worksheet_name)
course_notes_df = get_as_dataframe(worksheet=sheet, evaluate_formulas=True )

stacked_df = pd.merge( stacked_df,course_notes_df,left_on='COURSE',right_on='crse',how='left')

```

### Identify shared rooms

Many larger courses comprise of multiple CRN sharing a single classroom period.
We need to identify unique combinations of days of week and rooms and assign 
store these for later aggregation.

```{python}
meeting_cols = ['MON-IND','TUE-IND','WED-IND','THU-IND','FRI-IND','SAT-IND','SUN-IND','BEGIN TIME','END TIME','BUILDING','ROOM']
stacked_df["meeting_code"] = ""
for col in meeting_cols:
    stacked_df[col] = stacked_df[col].fillna('.')
    stacked_df["meeting_code"] = stacked_df["meeting_code"] + stacked_df[col].astype(str)
stacked_df['meeting_id'] = pd.factorize(stacked_df['meeting_code'])[0]

day_cols = ['MON-IND','TUE-IND','WED-IND','THU-IND','FRI-IND','SAT-IND','SUN-IND']
stacked_df["mtgs_per_wk"] = 0
for col in day_cols:
    stacked_df[col] = stacked_df[col].fillna('.')
    stacked_df['mtgs_per_wk'] = stacked_df["mtgs_per_wk"] + (stacked_df[col] != ".").astype(int)

```

### Compute appropriate aggregate measures

These measures are used in later calculations for numbers of courses, sections, instructors, etc.

```{python}

# Create various aggregate columns
stacked_df['sum_term'] = 1.0 / stacked_df.groupby(['TERM'])['TERM'].transform('count')
stacked_df['sum_term_crse'] = 1.0 / stacked_df.groupby(['TERM','COURSE'])['COURSE'].transform('count')
stacked_df['sum_term_crse_crn'] = 1.0 / stacked_df.groupby(['TERM','COURSE','CRN'])['CRN'].transform('count')
stacked_df['sum_term_crse_crn_students'] = stacked_df['ACTUAL ENROLLMENT'] * stacked_df['sum_term_crse_crn']
stacked_df['sum_term_crse_crn_hours'] =  stacked_df['sum_term_crse_crn_students'] * stacked_df['MAX CREDITS']

```


### Workload model assignments

These are placeholders for any proposed workload model based on course and instructor attributes.

The sort attribute is used to hold the aggregate workload score.  It is used in google sheets to help
sort courses by instructors in the proper order.

```{python}
stacked_df['sum_term_crse_wrkld_sample'] = 0.0
stacked_df['sum_term_crse_wrkld_a'] = 0.0
stacked_df['sum_term_crse_wrkld_b'] = 0.0
stacked_df['sum_term_crse_wrkld_c'] = 0.0
```

### Sample workload model

This is a sample workload model.  The assigned faculty workoad will be stored in the *sum_term_crse_wrklod_sample* field.  Here is the model:

1. Each section/CRN is worth a full teaching credit.
1. Research, indepedent study and coop/intern sections receive zero teaching credit in this workload model.  Instructors receive *credit* through their salary if they're staff.  Research active faculty get reduced teaching loads.
1. Lab sections including actual course labs, capstone/senior design, and VIP receive 1/3 credit.  This is consistent with the model that 3 lab hours is equivalent to 1 teaching hour. (e.g., 3-3-4 courses.)
NOTE that laboratory sections are NOT coded as separate labs, rather they are coded as LEC making it
difficult to discern these.  See EGRE306 for an example.
1. Seminar sections get full section credit as a positive incentive. There is a limited number of SEM courses, they are important to the curriculum, and we want them covered.
1. Capstone designs are scaled to give one LAB (0.33 per above) unit per groups of 4 students.


```{python}
# Assign standard CRN : One CRN/section per teaching course
stacked_df['sum_term_crse_wrkld_sample'] = stacked_df["sum_term_crse_crn"]

# Adjust these courses by shared offerings, classes with different CRN taught 
# by the same instructor in the same room/period
stacked_df['sum_term_crse_wrkld_sample'] = stacked_df['sum_term_crse_wrkld_sample'] / stacked_df.groupby(['TERM','instructor','meeting_id'])['meeting_id'].transform('count')

# For specific courses, if we see a meeting time with only one day per week then this is a lab section
# indepedent of the CRN.  We'll adjust them to LAB sections, then let the logic below
# code them as 1/3 credit, or whatever a lab section should get.

#courses_with_labs = ['EGRE306']
#for course in courses_with_labs:
#    stacked_df.loc[(stacked_df['COURSE']==course)&(stacked_df['mtgs_per_wk']==1),'TYPE'] = 'LAB'

# Hack to identify lab courses:  they are worth 4 units and coded as LEC. The
# meeting period that meets for 1 day per week is a LAB.
stacked_df.loc[(stacked_df['MAX CREDITS']==4)&(stacked_df['TYPE']=='LEC')&(stacked_df['mtgs_per_wk']==1),'TYPE'] = 'LAB'


# Exclude teaching credit for research (RES), independent study (IND), and intern/coop (FLD).
# Credit is given for these activities in reduced teaching (research active) or summer pay.
stacked_df.loc[stacked_df['TYPE'].isin(['RES','IND','FLD']),'sum_term_crse_wrkld_sample'] = 0.0

# Reassign LAB 1/3 of CRN.  Labs include capstone, regular lab sections, and VIP
stacked_df.loc[stacked_df['TYPE'].isin(['LAB']),'sum_term_crse_wrkld_sample'] = stacked_df.loc[stacked_df['TYPE'].isin(['LAB']),'sum_term_crse_wrkld_sample'] / 3.0

# For capstone senior design adjust workload to consider bundles of 4 students.  Each bundle is worth 0.33 (set above
senior_design_courses = ['CLSC403','EGRB401','EGRB402','CMSC441','CMSC442','CMSC451','CMSC452','EGMN402','EGMN403','ENGR402','ENGR403','EGRE404','EGRE405']
for course in senior_design_courses:
   stacked_df.loc[(stacked_df['COURSE']==course)&(stacked_df['TYPE']=='LAB'),'sum_term_crse_wrkld_sample'] = stacked_df.loc[(stacked_df['COURSE']==course)&(stacked_df['TYPE']=='LAB'),'sum_term_crse_wrkld_sample'] * stacked_df.loc[(stacked_df['COURSE']==course)&(stacked_df['TYPE']=='LAB'),'ACTUAL ENROLLMENT'].astype(int) / 4.0

# Ensure that SEM get full credit because we want to reward the faculty member for doing it!
stacked_df.loc[stacked_df['TYPE'].isin(['SEM']),'sum_term_crse_wrkld_sample'] = stacked_df.loc[stacked_df['TYPE'].isin(['SEM']),'sum_term_crse_wrkld_sample'] / 1.0

```

### Store the data for later use

We're storing both the stacked and unstacked data.  Note that the
aggregate measures are stored with the stacked data only.

```{python}
# Store as CSV files
sections_df.to_csv('sections_df.csv', index=False)
stacked_df.to_csv('stacked_df.csv', index=False)
```

### Store the data in google sheets

```{python}

# Open the worksheet 
spreadsheet_key = "1ZK7k8M85CXLof6FdeJYJuGFbfjsOXrCv5mc7OgUInWw"
worksheet_name = "Source data"

data_to_write = stacked_df.to_records(index=False)
try:
    sheet = client.open_by_key(spreadsheet_key).worksheet(worksheet_name)
except:
    sheet = client.open_by_key(spreadsheet_key).add_worksheet( 
        title = worksheet_name,nrows=100,ncols=10
    )
sheet.clear()
set_with_dataframe(worksheet=sheet, dataframe=stacked_df, include_index=False,include_column_header=True, resize=True)

```

### Freshen persistent instructor data

This block identifies any instructors not found in the persistent instructor data
and adds them to the list with default values.

```{python}
#| echo: true


#worksheet_name = "Instructor data"
#summary_df = stacked_df.groupby('instructor')[['COLLEGE','DEPT']].apply(lambda x: x.mode().iloc[0]).#reset_index()
#summary_df = summary_df.sort_values(by=['COLLEGE','DEPT','instructor'])
#data_to_write = summary_df.to_records(index=False)
#try:
#    sheet = client.open_by_key(spreadsheet_key).worksheet(worksheet_name)
#except:
#    nrows,ncols = summary_df.shape
#    sheet = client.open_by_key(spreadsheet_key).add_worksheet( 
#        title = worksheet_name,rows=nrows+1,cols=ncols+1
#    )
#sheet.clear()
#set_with_dataframe(worksheet=sheet, dataframe=summary_df, include_index=False,include_column_header=True, resize=True)

# Find names in df2 that are not in df1
##names_to_add = df2[~df2['name'].isin(df1['name'])]

# Add the rows with missing names from df2 to df1
## df1 = pd.concat([df1, names_to_add], ignore_index=True)

```
